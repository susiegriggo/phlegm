{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16dab2d8-de5b-40ae-b2af-1eec18b5ca13",
   "metadata": {},
   "source": [
    "# see if I can generate predictions using the queen predictor to see how they compare to the approach used in phlegm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25312d9b-a777-4fad-a147-88a5a4c537e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: fair-esm in /home/grig0076/miniconda3/envs/phynteny_transformer2/lib/python3.12/site-packages (2.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grig0076/miniconda3/envs/phynteny_transformer2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'QUEEN' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "!pip install fair-esm\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from esm import Alphabet, pretrained\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "import os.path\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from datetime import date\n",
    "import torch\n",
    "import re\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "import esm\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "from transformers import EsmTokenizer, EsmForSequenceClassification\n",
    "\n",
    "! git clone https://github.com/Furman-Lab/QUEEN/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8387d4-4d7a-424a-a22e-e165d4bda416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "model = EsmForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", problem_type=\"multi_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a64b156f-435a-4d51-b5af-9defd2ab4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6051516-54c1-4bac-ab95-90092afe3ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-32): 33 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ESM-2 model\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ada54b6-e34f-40b1-96f6-af67de8ab0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequences...\n",
      "Loaded 597 sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|▋         | 10/150 [07:25<1:45:57, 45.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40 sequences so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  13%|█▎        | 20/150 [14:38<1:35:18, 43.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80 sequences so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  20%|██        | 30/150 [22:00<1:33:25, 46.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 120 sequences so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  27%|██▋       | 40/150 [29:45<1:28:06, 48.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 160 sequences so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|███▎      | 50/150 [37:18<1:16:34, 45.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200 sequences so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|████      | 60/150 [45:16<1:08:43, 45.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 240 sequences so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  47%|████▋     | 70/150 [53:13<1:07:06, 50.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 280 sequences so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  53%|█████▎    | 80/150 [1:00:27<51:38, 44.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 320 sequences so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  60%|██████    | 90/150 [1:07:16<43:47, 43.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 360 sequences so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  67%|██████▋   | 100/150 [1:15:20<39:41, 47.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400 sequences so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  73%|███████▎  | 110/150 [1:23:11<29:47, 44.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 440 sequences so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  80%|████████  | 120/150 [1:31:28<25:59, 51.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 480 sequences so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  87%|████████▋ | 130/150 [1:39:36<15:57, 47.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 520 sequences so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  93%|█████████▎| 140/150 [1:46:27<07:27, 44.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 560 sequences so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 150/150 [1:53:52<00:00, 45.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 597 sequences so far...\n",
      "Saving 597 embeddings to ESM_embed.pkl\n",
      "Processing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "def process_sequences_in_batches(fasta_file_path, model, batch_converter, alphabet, \n",
    "                               batch_size=32, output_file=\"ESM_embed.pkl\"):\n",
    "    \"\"\"\n",
    "    Process protein sequences in batches to generate ESM embeddings efficiently.\n",
    "    \n",
    "    Args:\n",
    "        fasta_file_path: Path to the FASTA file\n",
    "        model: ESM model\n",
    "        batch_converter: ESM batch converter\n",
    "        alphabet: ESM alphabet\n",
    "        batch_size: Number of sequences to process at once\n",
    "        output_file: Output pickle file path\n",
    "    \"\"\"\n",
    "    \n",
    "    # First pass: collect all sequences\n",
    "    print(\"Loading sequences...\")\n",
    "    all_data = []\n",
    "    for record in SeqIO.parse(fasta_file_path, \"fasta\"):\n",
    "        all_data.append((record.id, str(record.seq)))\n",
    "    \n",
    "    print(f\"Loaded {len(all_data)} sequences\")\n",
    "    \n",
    "    # Process in batches\n",
    "    sequence_representations = []\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    for i in tqdm(range(0, len(all_data), batch_size), desc=\"Processing batches\"):\n",
    "        batch_data = all_data[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            # Convert batch\n",
    "            batch_labels, batch_strs, batch_tokens = batch_converter(batch_data)\n",
    "            batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "            \n",
    "            # Process with model\n",
    "            with torch.no_grad():\n",
    "                results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "            \n",
    "            token_representations = results[\"representations\"][33]\n",
    "            \n",
    "            # Extract per-residue representations for this batch\n",
    "            for j, tokens_len in enumerate(batch_lens):\n",
    "                # Average pooling to get sequence-level representation\n",
    "                seq_repr = token_representations[j, 1:tokens_len-1].mean(0)\n",
    "                sequence_representations.append(seq_repr)\n",
    "            \n",
    "            # Clean up GPU memory after each batch\n",
    "            del batch_tokens, results, token_representations\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"OOM error at batch starting at index {i}. Consider reducing batch_size.\")\n",
    "                print(f\"Current batch size: {batch_size}, sequences in this batch: {len(batch_data)}\")\n",
    "                # Optionally, you could retry with a smaller batch size\n",
    "                raise e\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        # Optional: save intermediate results every N batches\n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"Processed {len(sequence_representations)} sequences so far...\")\n",
    "    \n",
    "    # Save all representations\n",
    "    print(f\"Saving {len(sequence_representations)} embeddings to {output_file}\")\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(sequence_representations, f)\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "    return sequence_representations\n",
    "\n",
    "# Alternative: Memory-efficient streaming version\n",
    "def process_sequences_streaming(fasta_file_path, model, batch_converter, alphabet, \n",
    "                              batch_size=32, output_file=\"ESM_embed.pkl\"):\n",
    "    \"\"\"\n",
    "    Memory-efficient streaming version that doesn't load all sequences into memory first.\n",
    "    \"\"\"\n",
    "    \n",
    "    sequence_representations = []\n",
    "    batch_data = []\n",
    "    \n",
    "    print(\"Processing sequences in streaming mode...\")\n",
    "    \n",
    "    with tqdm(desc=\"Processing sequences\") as pbar:\n",
    "        for record in SeqIO.parse(fasta_file_path, \"fasta\"):\n",
    "            batch_data.append((record.id, str(record.seq)))\n",
    "            \n",
    "            # Process when batch is full\n",
    "            if len(batch_data) == batch_size:\n",
    "                sequence_representations.extend(\n",
    "                    process_batch(batch_data, model, batch_converter, alphabet)\n",
    "                )\n",
    "                batch_data = []  # Reset batch\n",
    "                pbar.update(batch_size)\n",
    "        \n",
    "        # Process remaining sequences\n",
    "        if batch_data:\n",
    "            sequence_representations.extend(\n",
    "                process_batch(batch_data, model, batch_converter, alphabet)\n",
    "            )\n",
    "            pbar.update(len(batch_data))\n",
    "    \n",
    "    # Save results\n",
    "    print(f\"Saving {len(sequence_representations)} embeddings to {output_file}\")\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(sequence_representations, f)\n",
    "    \n",
    "    return sequence_representations\n",
    "\n",
    "def process_batch(batch_data, model, batch_converter, alphabet):\n",
    "    \"\"\"Helper function to process a single batch.\"\"\"\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(batch_data)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "    \n",
    "    token_representations = results[\"representations\"][33]\n",
    "    batch_representations = []\n",
    "    \n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        seq_repr = token_representations[i, 1:tokens_len-1].mean(0)\n",
    "        batch_representations.append(seq_repr)\n",
    "    \n",
    "    # Clean up\n",
    "    del batch_tokens, results, token_representations\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return batch_representations\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    fasta_file_path = '/home/grig0076/scratch/phlegm/PHROGs/phrog_representative_structures/monomers/fasta_known_state.faa'\n",
    "    \n",
    "    # Choose batch size based on your GPU memory\n",
    "    # Start with 32, reduce if you get OOM errors\n",
    "    batch_size = 4\n",
    "    \n",
    "    # Use the regular batching approach\n",
    "    embeddings = process_sequences_in_batches(\n",
    "        fasta_file_path, model, batch_converter, alphabet, \n",
    "        batch_size=batch_size, output_file=\"ESM_embed.pkl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb999faf-181c-43f0-8b2e-6573a403e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if the embeddings object is the same as the sequence representations object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "751605b9-86e8-4b18-902a-f0ea380a6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not previous tab, we generate it here:\n",
    "full_tab_for_embed = pd.DataFrame()\n",
    "np_list = []\n",
    "# Detach the tensors to obtain a numpy array\n",
    "for i, ten in enumerate(embeddings):\n",
    "  ten=ten.detach().numpy()\n",
    "  np_list.append(ten)\n",
    "full_tab_for_embed[\"esm_embeddings\"] = pd.Series(np_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44979dbb-60e3-402e-a70d-b00090e035a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grig0076/miniconda3/envs/phynteny_transformer2/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelBinarizer from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/grig0076/miniconda3/envs/phynteny_transformer2/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MLPClassifier from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model_location = \"QUEEN/QUEEN_MLPmodel_final.pkl\"\n",
    "with open(model_location, \"rb\") as f:\n",
    "  QUEEN_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e3ac0aa-da34-43b9-ae23-1802e15af69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the predicted labels:\n",
      "[ 3  2  1  1  2  2  2  1  8  2  2  2  1  1  2  1  1  2  2  2  2  2  1  2\n",
      "  1  2  2  1  3  1  2  2  1  3  2 12  1  3  1  1  2  3  1  2  1  2  2 12\n",
      "  1  2  1  2  1  2  1  2  1  4  1  2  2  4  2  3  4  2  1  1  2  3  1  1\n",
      "  8  4  2  5  2  2 12  1  2  3  2  3  2  8  4  2  8  3 12  2  1  6  1  1\n",
      "  2  2  4  1  1  2  8  8  2  1  2  1  3  1  2  4  6  2  2  4  2  1  4  3\n",
      "  1  1  1  1  3  2  2  1  6  3  3  2 24  6  1  2  4  2  1  6  1 12  1  5\n",
      "  1  2  2  1  2  2  2  6  1  1  1  2  1  3  1  1  2  1  1  6  2  3  1  2\n",
      "  2 12  1  1  2  2  2  1  1  4  1  8  3  2  2  1  1  8  2  1 10  1  4  1\n",
      "  1  8  2 12 12  8  5  3  3  4  2  6  4  1  1  1  4  1  1  1  1  3  1  2\n",
      "  8  2  1  2  6  2  1  2  1  3  1  1  1  1  8  1  1  3  2  2  2  2  2  2\n",
      "  2  1 12  1  1  1  6  2  2  2  2  2  2  2  1  2  1  2  2  2  3  2  3  8\n",
      "  8  1  1  2  8  2  2  2  8  3  1  2 12  2  2  3  1  3  4  4  2  2  6  1\n",
      "  2  2  1  2  3  3  2  4  2  1  2  1  3  3  1  6  1  3  2  2  8  1  2  4\n",
      "  4  2  6  4  5 12  2  3  2  2  3  1  1  4  2  2  2  1  1  6  1  8  1  2\n",
      "  4  1  2  6  6  1  2  2  1  1  6  2  1  1  7  2  2  4  1  1  6  1  2  1\n",
      "  2  2  1  8  1  4  1  1  2  1  1  4  2  2  2  2  2  3  2  8  1  2  1  1\n",
      "  1  2  6  2  5  1  2  8  1  2  1  8  3  2  5  3  6  2  2  1  1  6  4  3\n",
      " 12  2  3 12  1  2  2  2  2  2  8  2  6  2  1  4  2  2  2  4  2  2  3  1\n",
      "  4  3  1 24  1  1 10  2  8  4  8  1  1  4  1  4  2  6  1  2  8  2  1  2\n",
      "  4  4 12  2  1  1  4  1  1  2  1  2  1  1  1  6 12  2  1  3  2  2  1  1\n",
      "  6  1  2  4  2  2  1  3  2  5  1  2  3  8  2  2  1  1  2  8  2  1  1  2\n",
      "  1  2  3  1  2  1  1  2  2  6  8  2  1  4  1  1  1  2  4  3  2  2  2  6\n",
      "  2  1  2  6  6  2  1  2  4  2  1  4  2  1  2  1  2  1  1  2  1  2  2  2\n",
      "  4  1  1  3  3  2  4  1  6  1  4  1  3  2  2  2  6  1  1  2  2  2  2  1\n",
      "  2  1 12  3  1  1  4  2  1  2  3  2  4  1  3  2  4  1  1  2  1]\n"
     ]
    }
   ],
   "source": [
    "y_test = QUEEN_model.predict(full_tab_for_embed[\"esm_embeddings\"].to_list())\n",
    "y_probs = QUEEN_model.predict_proba(full_tab_for_embed[\"esm_embeddings\"].to_list())\n",
    "y_predict_probs = [np.max(i) for i in y_probs] \n",
    "inv_map = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 10, 9: 12, 10: 14, 11: 24}\n",
    "y_test_transformed = np.array([inv_map[x] for x in y_test])\n",
    "print(\"These are the predicted labels:\")\n",
    "print(y_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1905ff8c-cc04-4a42-bb76-9ff478256c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the headers so that these can be mapped across \n",
    "# Read headers into a list\n",
    "headers = [record.id for record in SeqIO.parse(fasta_file_path, \"fasta\")] \n",
    "queen_results = pd.DataFrame({'phrog': headers, 'num_subunits': y_test_transformed, 'probability': y_predict_probs})\n",
    "queen_results.to_csv('queen_phrog_validation_set_predictions.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "44651415-1596-4a95-95bd-dd4e6ae9870d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8462631837770839"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de317c6b-de37-4c8e-86f7-ea30fca7acd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
